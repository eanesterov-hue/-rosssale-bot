#!/usr/bin/env python3
"""
Telegram-–±–æ—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –±—Ä–æ–∫–µ—Ä–æ–≤ –ø–æ –æ–±—ä–µ–∫—Ç—É –∏ —Ä–∞–π–æ–Ω—É
"""
import os
import sys
import json
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "scripts"))

from dotenv import load_dotenv
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import pandas as pd
from rapidfuzz import fuzz, process
from datetime import datetime, timedelta

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv(PROJECT_ROOT / ".env")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DATA_FILE = PROJECT_ROOT / "data" / "showings.xlsx"
DISTRICTS_FILE = PROJECT_ROOT / "data" / "districts.json"
DAYS = 60  # 2 –º–µ—Å—è—Ü–∞

# –ì—Ä—É–ø–ø—ã —Å–∏–Ω–æ–Ω–∏–º–æ–≤: –æ–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –æ–¥–Ω–∏–º –ñ–ö
# –ü—Ä–∏ –ø–æ–∏—Å–∫–µ –æ–¥–Ω–æ–≥–æ ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –±—Ä–æ–∫–µ—Ä–æ–≤ –ø–æ –≤—Å–µ–º –∏–∑ –≥—Ä—É–ø–ø—ã
SYNONYMS = [
    # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ ‚Üî –õ–∞—Ç–∏–Ω–∏—Ü–∞
    ["–ü—Ä–∞–π–º –ø–∞—Ä–∫", "Prime Park"],
    ["–®–∞–≥–∞–ª", "Shagal"],
    ["–°–æ—É–ª", "Soul"],
    ["–°–ª–∞–≤–∞", "Slava"],
    ["–†–∞–∫—É—Ä—Å", "Rakurs"],
    ["–ü—Ä–∏–Ω—Ü–∏–ø–∞–ª –ø–ª–∞–∑–∞", "Principal Plaza"],
    ["–ü—Ä–∏–º–∞–≤–µ—Ä–∞ –Ω–æ–≤–∞—è", "Primavera"],
    ["–°–∏–Ω–∞—Ç—Ä–∞", "Sinatra –≤—Ç–æ—Ä–∏—á–∫–∞"],
    ["–ë–∞–ª—á—É–≥ —Ä–µ–∑–∏–¥–µ–Ω—Å", "Balchug Residence"],
    ["–°–∏–¥–Ω–µ–π –°–∏—Ç–∏", "Sidney City", "Sydney city"],
    
    # –í—Ç–æ—Ä–∏—á–∫–∞ = –ü–µ—Ä–≤–∏—á–∫–∞
    ["–ë–∞—à–Ω—è –§–µ–¥–µ—Ä–∞—Ü–∏—è", "–ë–∞—à–Ω—è –§–µ–¥–µ—Ä–∞—Ü–∏—è –≤—Ç–æ—Ä–∏—á–∫–∞"],
    ["–°–∞–¥–æ–≤—ã–µ –∫–≤–∞—Ä—Ç–∞–ª—ã", "–í—Ç–æ—Ä–∏—á–∫–∞ –°–∞–¥–æ–≤—ã–µ –∫–≤–∞—Ä—Ç–∞–ª—ã"],
    ["–î–∏–Ω–∞—Å—Ç–∏—è", "–í—Ç–æ—Ä–∏—á–∫–∞ –î–∏–Ω–∞—Å—Ç–∏—è"],
    ["Knightsbridge Private Park", "–í—Ç–æ—Ä–∏—á–∫–∞ Knightsbridge Private Park"],
    ["–û—Å—Ç—Ä–æ–≤", "–û—Å—Ç—Ä–æ–≤ –í—Ç–æ—Ä–∏—á–∫–∞"],
    ["–ñ–ö –ö—Ä—ã–ª—å—è", "–ö—Ä—ã–ª—å—è –≤—Ç–æ—Ä–∏—á–∫–∞"],
    
    # –†–∞–∑–Ω—ã–µ –Ω–∞–ø–∏—Å–∞–Ω–∏—è
    ["–ñ–ö –¢–∞–≤—Ä–∏—á–µ—Å–∫–∏–π", "–¢–∞–≤—Ä–∏—á–µ—Å–∫–∏–π"],
    ["–î–æ–º –≤ –ù–∏–∫–æ–ª–∏–Ω–æ", "–ù–∏–∫–æ–ª–∏–Ω–æ"],
    ["–ë–∞—à–Ω—è –ì–æ—Ä–æ–¥ –°—Ç–æ–ª–∏—Ü", "–ì–æ—Ä–æ–¥ –°—Ç–æ–ª–∏—Ü"],
    ["Level –ú–∏—á—É—Ä–∏–Ω—Å–∫–∏–π", "–ú–∏—á—É—Ä–∏–Ω—Å–∫–∏–π"],
    ["Canal Front", "Canal Front Residences 3"],
    ["–ü–æ–∫–ª–æ–Ω–Ω–∞—è 9", "–ü–æ–∫–ª–æ–Ω–Ω–∞—è, 9", "–ü–æ–∫–ª–∞–Ω–Ω–∞—è 9"],  # + –æ–ø–µ—á–∞—Ç–∫–∞
    
    # –û–ø–µ—á–∞—Ç–∫–∏
    ["Lucky", "Lacky"],
]

# –°–ª–æ–≤–∞—Ä—å –∞–ª–∏–∞—Å–æ–≤: –∑–∞–ø—Ä–æ—Å ‚Üí –Ω–∞ —á—Ç–æ –∑–∞–º–µ–Ω–∏—Ç—å
# –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è, –æ–ø–µ—á–∞—Ç–∫–∏, –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø–∏—Å–∞–Ω–∏—è
ALIASES = {
    # –§–æ–Ω–µ—Ç–∏–∫–∞: –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ ‚Üí –ª–∞—Ç–∏–Ω–∏—Ü–∞
    "–∫–ª–∞—É–¥": "cloud",
    "–∫–ª–∞—É–¥ —Ç–∞—É—ç—Ä": "cloud tower",
    "—Ç–∞—É—ç—Ä": "tower",
    "–≥—Ä–∞–Ω–¥": "grand",
    "–≥–∞—Ä–¥–µ–Ω": "garden",
    "–≤–µ—Å—Ç –≥–∞—Ä–¥–µ–Ω": "west garden",
    "–≤–µ—Å—Ç": "west",
    "—Ä–µ–∑–∏–¥–µ–Ω—Å": "residences",
    "–ø–∏–Ω–Ω–∞–∫–ª": "pinnacle",
    "–º–∞—Ä–∏–Ω–∞": "marina",
    "–∫–∞–Ω–∞–ª": "canal",
    "—Ñ—Ä–æ–Ω—Ç": "front",
    "–ø–∞–Ω–æ—Ä–∞–º–∏–∫": "panoramic",
    "—Å—Ç–µ–ª–ª–∞": "stella",
    "–º–∞—Ä–∏—Å": "maris",
    "–≤–∏–¥–∞": "vida",
    "–∫—Ä–∏–∫": "creek",
    "–±–∏—á": "beach",
    
    # –û–ø–µ—á–∞—Ç–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö
    "–ø–æ–∫–ª–æ–Ω–Ω–∞—è": "–ø–æ–∫–ª–∞–Ω–Ω–∞—è",  # –≤ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø–µ—á–∞—Ç–∫–æ–π
    
    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø–∏—Å–∞–Ω–∏—è
    "–ø—Ä–∞–π–º–ø–∞—Ä–∫": "–ø—Ä–∞–π–º –ø–∞—Ä–∫",
    "–ø—Ä–∞–π–º": "–ø—Ä–∞–π–º –ø–∞—Ä–∫",
    "–∞—Ä—Ç—Ö–∞—É—Å": "–∞—Ä—Ç—Ö–∞—É—Å",
    "–≤–µ–ª–ª—Ç–æ–Ω": "–≤–µ–ª–ª—Ç–æ–Ω —Ç–∞—É—ç—Ä—Å",
    "–∫–≤–∞—Ä—Ç–∞–ª –Ω–∞ –ª–µ–Ω–∏–Ω—Å–∫–æ–º": "–∂–∫ –∫–≤–∞—Ä—Ç–∞–ª –Ω–∞ –ª–µ–Ω–∏–Ω—Å–∫–æ–º",
}


def transliterate_ru_to_en(text: str) -> str:
    """–¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –≤ –ª–∞—Ç–∏–Ω–∏—Ü—É"""
    table = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = ""
    for char in text.lower():
        result += table.get(char, char)
    return result


def normalize(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏"""
    if pd.isna(text):
        return ""
    text = str(text).strip().lower()
    while "  " in text:
        text = text.replace("  ", " ")
    return text


def normalize_for_search(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ ‚Äî —Å —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–µ–π"""
    text = normalize(text)
    # –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∏—Ä—É–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É
    return transliterate_ru_to_en(text)


def apply_aliases(query: str) -> list:
    """
    –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å –∞–ª–∏–∞—Å–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞.
    """
    query_norm = normalize(query)
    variants = [query_norm]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –∞–ª–∏–∞—Å–æ–º
    if query_norm in ALIASES:
        variants.append(normalize(ALIASES[query_norm]))
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –ª–∏ –∞–ª–∏–∞—Å –≤ –∑–∞–ø—Ä–æ—Å–µ
    for alias, replacement in ALIASES.items():
        if alias in query_norm and alias != query_norm:
            new_query = query_norm.replace(alias, replacement)
            if new_query not in variants:
                variants.append(new_query)
    
    return variants


def get_synonyms(object_name: str) -> list:
    """
    –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Å–∏–Ω–æ–Ω–∏–º—ã –æ–±—ä–µ–∫—Ç–∞ (–≤–∫–ª—é—á–∞—è —Å–∞–º –æ–±—ä–µ–∫—Ç).
    """
    object_norm = normalize(object_name)
    
    for group in SYNONYMS:
        group_norm = [normalize(o) for o in group]
        if object_norm in group_norm:
            return group  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å—é –≥—Ä—É–ø–ø—É
    
    return [object_name]  # –ù–µ—Ç —Å–∏–Ω–æ–Ω–∏–º–æ–≤ ‚Äî —Ç–æ–ª—å–∫–æ —Å–∞–º –æ–±—ä–µ–∫—Ç


def load_districts():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ä–∞–π–æ–Ω–æ–≤"""
    if not DISTRICTS_FILE.exists():
        return {}
    
    with open(DISTRICTS_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    return data.get("objects", {})


def get_objects_by_district(district_query: str) -> list:
    """
    –ù–∞–π—Ç–∏ –≤—Å–µ –ñ–ö –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ä–∞–π–æ–Ω–µ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤.
    """
    districts_data = load_districts()
    district_query_norm = normalize(district_query)
    
    results = []
    
    for obj_name, info in districts_data.items():
        district_norm = normalize(info.get("district", ""))
        city_norm = normalize(info.get("city", ""))
        
        # –ò—â–µ–º –ø–æ —Ä–∞–π–æ–Ω—É (fuzzy)
        if district_query_norm in district_norm or district_norm in district_query_norm:
            results.append({
                "object": obj_name,
                "district": info.get("district"),
                "city": info.get("city"),
                "country": info.get("country")
            })
        # –¢–∞–∫–∂–µ –∏—â–µ–º –ø–æ –≥–æ—Ä–æ–¥—É (–¥–ª—è –î—É–±–∞—è ‚Äî –º–æ–∂–Ω–æ –∏—Å–∫–∞—Ç—å "–î—É–±–∞–π")
        elif district_query_norm in city_norm or city_norm == district_query_norm:
            results.append({
                "object": obj_name,
                "district": info.get("district"),
                "city": info.get("city"),
                "country": info.get("country")
            })
    
    return results


def get_all_districts() -> list:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–∞–π–æ–Ω–æ–≤"""
    districts_data = load_districts()
    districts_set = set()
    
    for obj_name, info in districts_data.items():
        district = info.get("district")
        city = info.get("city")
        if district and city:
            districts_set.add(f"{district} ({city})")
    
    return sorted(districts_set)


def load_data():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞"""
    if not DATA_FILE.exists():
        return None, "–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω"
    
    df = pd.read_excel(DATA_FILE)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–æ–Ω–æ–∫
    required = ["–ë—Ä–æ–∫–µ—Ä", "–î–∞—Ç–∞", "–û–±—ä–µ–∫—Ç"]
    for col in required:
        if col not in df.columns:
            return None, f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞: {col}"
    
    return df, None


def parse_date(date_str: str) -> datetime:
    """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã"""
    return datetime.strptime(date_str, "%d.%m.%Y")


def find_best_match(query: str, objects: list) -> tuple:
    """
    –ù–∞–π—Ç–∏ –±–ª–∏–∂–∞–π—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–Ω–∞–π–¥–µ–Ω–Ω—ã–π_–æ–±—ä–µ–∫—Ç, score, exact_match) –∏–ª–∏ (None, 0, False)
    """
    if not objects:
        return None, 0, False
    
    # –ü–æ–ª—É—á–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ –∞–ª–∏–∞—Å—ã
    query_variants = apply_aliases(query)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤
    objects_norm = [normalize(o) for o in objects]
    objects_translit = [normalize_for_search(o) for o in objects]
    
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    for query_variant in query_variants:
        query_translit = normalize_for_search(query_variant)
        
        # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for i, obj in enumerate(objects):
            if query_variant == objects_norm[i] or query_translit == objects_translit[i]:
                return obj, 100, True
        
        # 2. –í—Ö–æ–∂–¥–µ–Ω–∏–µ (contains) ‚Äî –∑–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –æ–±—ä–µ–∫—Ç–µ
        for i, obj in enumerate(objects):
            if query_variant in objects_norm[i] or query_translit in objects_translit[i]:
                return obj, 95, True
        
        # 3. –û–±—Ä–∞—Ç–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ ‚Äî –æ–±—ä–µ–∫—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –∑–∞–ø—Ä–æ—Å–µ
        for i, obj in enumerate(objects):
            if objects_norm[i] in query_variant or objects_translit[i] in query_translit:
                return obj, 90, True
    
    # 4. Fuzzy search —Å —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–µ–π (–ø–æ—Ä–æ–≥ 75) ‚Äî —Ç–æ–ª—å–∫–æ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ
    best_result = None
    best_score = 0
    
    for query_variant in query_variants:
        query_translit = normalize_for_search(query_variant)
        result = process.extractOne(
            query_translit,
            objects_translit,
            scorer=fuzz.WRatio,
            score_cutoff=75  # –≤—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥ ‚Äî —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏–µ
        )
        
        if result and result[1] > best_score:
            best_result = result
            best_score = result[1]
    
    if best_result:
        match_translit, score, idx = best_result
        return objects[idx], score, False
    
    return None, 0, False


def search_brokers(query: str) -> dict:
    """
    –ü–æ–∏—Å–∫ –±—Ä–æ–∫–µ—Ä–æ–≤ –ø–æ –æ–±—ä–µ–∫—Ç—É —Å fuzzy matching.
    """
    df, error = load_data()
    if error:
        return {"error": error}
    
    # –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ
    cutoff = datetime.now() - timedelta(days=DAYS)
    df["_date"] = df["–î–∞—Ç–∞"].apply(parse_date)
    df = df[df["_date"] >= cutoff]
    
    if df.empty:
        return {"error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥"}
    
    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
    objects = df["–û–±—ä–µ–∫—Ç"].dropna().unique().tolist()
    
    # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–µ–≥–æ –æ–±—ä–µ–∫—Ç–∞
    best_match, score, exact = find_best_match(query, objects)
    
    if not best_match:
        return {
            "found": False,
            "query": query,
            "days": DAYS
        }
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å–∏–Ω–æ–Ω–∏–º—ã –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
    synonyms = get_synonyms(best_match)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Å–µ–º —Å–∏–Ω–æ–Ω–∏–º–∞–º
    df_filtered = df[df["–û–±—ä–µ–∫—Ç"].isin(synonyms)]
    
    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –±—Ä–æ–∫–µ—Ä—ã
    brokers = df_filtered["–ë—Ä–æ–∫–µ—Ä"].dropna().str.strip()
    brokers = brokers[brokers != ""]
    brokers = sorted(brokers.unique())
    
    # –ö–∞–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã —Ä–µ–∞–ª—å–Ω–æ –Ω–∞—à–ª–∏—Å—å –≤ –¥–∞–Ω–Ω—ã—Ö
    found_objects = df_filtered["–û–±—ä–µ–∫—Ç"].unique().tolist()
    
    return {
        "found": True,
        "query": query,
        "object": best_match,
        "objects": found_objects,  # –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        "days": DAYS,
        "brokers": brokers,
        "score": score,
        "exact": exact  # —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ fuzzy
    }


def search_by_district(district_query: str) -> dict:
    """
    –ü–æ–∏—Å–∫ –±—Ä–æ–∫–µ—Ä–æ–≤ –ø–æ —Ä–∞–π–æ–Ω—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –ñ–ö.
    """
    df, error = load_data()
    if error:
        return {"error": error}
    
    # –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ
    cutoff = datetime.now() - timedelta(days=DAYS)
    df["_date"] = df["–î–∞—Ç–∞"].apply(parse_date)
    df = df[df["_date"] >= cutoff]
    
    if df.empty:
        return {"error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥"}
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ñ–ö –≤ —ç—Ç–æ–º —Ä–∞–π–æ–Ω–µ –∏–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞
    district_objects = get_objects_by_district(district_query)
    
    if not district_objects:
        # –ü—Ä–æ–±—É–µ–º fuzzy –ø–æ–∏—Å–∫ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ä–∞–π–æ–Ω–∞
        all_districts = get_all_districts()
        district_query_norm = normalize(district_query)
        
        # Fuzzy –ø–æ–∏—Å–∫
        result = process.extractOne(
            district_query_norm,
            [normalize(d) for d in all_districts],
            scorer=fuzz.WRatio,
            score_cutoff=70
        )
        
        if result:
            match_norm, score, idx = result
            suggested = all_districts[idx]
            return {
                "found": False,
                "query": district_query,
                "suggestion": suggested,
                "days": DAYS
            }
        
        return {
            "found": False,
            "query": district_query,
            "days": DAYS
        }
    
    # –°–æ–±–∏—Ä–∞–µ–º –∏–º–µ–Ω–∞ –ñ–ö –∏–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞
    object_names = [obj["object"] for obj in district_objects]
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —ç—Ç–∏–º –æ–±—ä–µ–∫—Ç–∞–º
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    objects_norm = {normalize(name): name for name in object_names}
    df["_obj_norm"] = df["–û–±—ä–µ–∫—Ç"].apply(normalize)
    
    # –ù–∞—Ö–æ–¥–∏–º –æ–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
    df_filtered = df[df["_obj_norm"].isin(objects_norm.keys())]
    
    if df_filtered.empty:
        # –†–∞–π–æ–Ω –µ—Å—Ç—å –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–µ, –Ω–æ –ø–æ–∫–∞–∑–æ–≤ –Ω–µ—Ç
        district_info = district_objects[0]  # –ë–µ—Ä—ë–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–π–æ–Ω–µ
        return {
            "found": True,
            "no_showings": True,
            "query": district_query,
            "district": district_info.get("district"),
            "city": district_info.get("city"),
            "objects_in_district": object_names,
            "days": DAYS
        }
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ñ–ö –∏ —Å–æ–±–∏—Ä–∞–µ–º –±—Ä–æ–∫–µ—Ä–æ–≤
    results_by_object = {}
    
    for _, row in df_filtered.iterrows():
        obj_norm = row["_obj_norm"]
        obj_name = objects_norm.get(obj_norm, row["–û–±—ä–µ–∫—Ç"])
        broker = str(row["–ë—Ä–æ–∫–µ—Ä"]).strip() if pd.notna(row["–ë—Ä–æ–∫–µ—Ä"]) else ""
        
        if not broker:
            continue
            
        if obj_name not in results_by_object:
            results_by_object[obj_name] = set()
        results_by_object[obj_name].add(broker)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    district_info = district_objects[0]
    
    return {
        "found": True,
        "query": district_query,
        "district": district_info.get("district"),
        "city": district_info.get("city"),
        "days": DAYS,
        "by_object": {obj: sorted(brokers) for obj, brokers in results_by_object.items()},
        "total_brokers": len(set().union(*results_by_object.values())) if results_by_object else 0
    }


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /start"""
    await update.message.reply_text(
        "üëã –ü—Ä–∏–≤–µ—Ç!\n\n"
        "–ù–∞–ø–∏—à–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –ñ–ö, –∏ —è –ø–æ–∫–∞–∂—É, –∫–∞–∫–∏–µ –±—Ä–æ–∫–µ—Ä—ã –≤–µ–ª–∏ —Ç–∞–º –ø–æ–∫–∞–∑—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 –¥–Ω–µ–π.\n\n"
        "–ü—Ä–∏–º–µ—Ä—ã:\n"
        "‚Ä¢ –ü—Ä–∞–π–º –ø–∞—Ä–∫ ‚Äî –ø–æ–∏—Å–∫ –ø–æ –ñ–ö\n"
        "‚Ä¢ —Ä–∞–π–æ–Ω –•–∞–º–æ–≤–Ω–∏–∫–∏ ‚Äî –ø–æ–∏—Å–∫ –ø–æ —Ä–∞–π–æ–Ω—É"
    )


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /help"""
    await update.message.reply_text(
        "üîç –ö–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è:\n\n"
        "**–ü–æ –ñ–ö:** –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ\n"
        "–ü—Ä–∏–º–µ—Ä: –ü—Ä–∞–π–º –ø–∞—Ä–∫\n\n"
        "**–ü–æ —Ä–∞–π–æ–Ω—É:** –Ω–∞–ø–∏—à–∏ ¬´—Ä–∞–π–æ–Ω¬ª + –Ω–∞–∑–≤–∞–Ω–∏–µ\n"
        "–ü—Ä–∏–º–µ—Ä: —Ä–∞–π–æ–Ω –•–∞–º–æ–≤–Ω–∏–∫–∏\n"
        "–ü—Ä–∏–º–µ—Ä: —Ä–∞–π–æ–Ω Business Bay\n\n"
        "–ú–æ–∂–Ω–æ –ø–∏—Å–∞—Ç—å —Å –æ–ø–µ—á–∞—Ç–∫–∞–º–∏ ‚Äî —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å —É–≥–∞–¥–∞—Ç—å üòâ",
        parse_mode="Markdown"
    )


async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    query = update.message.text.strip()
    
    if not query:
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º: –ø–æ–∏—Å–∫ –ø–æ —Ä–∞–π–æ–Ω—É?
    query_lower = query.lower()
    if query_lower.startswith("—Ä–∞–π–æ–Ω ") or query_lower.startswith("—Ä–∞–π–æ–Ω:"):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–π–æ–Ω–∞
        district_query = query[6:].strip().lstrip(":")
        if district_query:
            await handle_district_search(update, district_query)
            return
    
    # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –ñ–ö
    result = search_brokers(query)
    
    if "error" in result:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")
        return
    
    if not result["found"]:
        await update.message.reply_text(
            f"üîç –ü–æ –∑–∞–ø—Ä–æ—Å—É ¬´{result['query']}¬ª –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\n\n"
            f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
        )
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º: —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ fuzzy
    if not result.get("exact", True):
        # Fuzzy match ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¢–û–õ–¨–ö–û –ø–æ–¥—Å–∫–∞–∑–∫—É, –±–µ–∑ –±—Ä–æ–∫–µ—Ä–æ–≤
        await update.message.reply_text(
            f"üîç –¢–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\n\n"
            f"–í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: **{result['object']}**?\n\n"
            f"–í–≤–µ–¥–∏—Ç–µ —Ç–æ—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞.",
            parse_mode="Markdown"
        )
        return
    
    # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –±—Ä–æ–∫–µ—Ä–æ–≤
    brokers_list = "\n".join([f"‚Ä¢ {b}" for b in result["brokers"]])
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ–±—ä–µ–∫—Ç–∞ (–µ—Å–ª–∏ —Å–∏–Ω–æ–Ω–∏–º—ã)
    found_objects = result.get("objects", [result["object"]])
    if len(found_objects) > 1:
        objects_str = " / ".join(found_objects)
        header = f"üè† {objects_str}"
    else:
        header = f"üè† {result['object']}"
    
    response = (
        f"{header}\n\n"
        f"–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {result['days']} –¥–Ω–µ–π –ø–æ–∫–∞–∑—ã –≤–µ–ª–∏:\n"
        f"{brokers_list}"
    )
    
    await update.message.reply_text(response)


async def handle_district_search(update: Update, district_query: str):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —Ä–∞–π–æ–Ω—É"""
    result = search_by_district(district_query)
    
    if "error" in result:
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")
        return
    
    if not result["found"]:
        # –ù–µ –Ω–∞—à–ª–∏ —Ä–∞–π–æ–Ω
        if "suggestion" in result:
            await update.message.reply_text(
                f"üîç –†–∞–π–æ–Ω ¬´{result['query']}¬ª –Ω–µ –Ω–∞–π–¥–µ–Ω.\n\n"
                f"–í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: **{result['suggestion']}**?",
                parse_mode="Markdown"
            )
        else:
            await update.message.reply_text(
                f"üîç –†–∞–π–æ–Ω ¬´{result['query']}¬ª –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–µ.\n\n"
                f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
        return
    
    # –†–∞–π–æ–Ω –Ω–∞–π–¥–µ–Ω, –Ω–æ –Ω–µ—Ç –ø–æ–∫–∞–∑–æ–≤
    if result.get("no_showings"):
        objects_list = "\n".join([f"‚Ä¢ {obj}" for obj in result["objects_in_district"][:10]])
        if len(result["objects_in_district"]) > 10:
            objects_list += f"\n...–∏ –µ—â—ë {len(result['objects_in_district']) - 10}"
        
        await update.message.reply_text(
            f"üìç **{result['district']}** ({result['city']})\n\n"
            f"–ñ–ö –≤ —ç—Ç–æ–º —Ä–∞–π–æ–Ω–µ:\n{objects_list}\n\n"
            f"‚ùå –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {result['days']} –¥–Ω–µ–π –ø–æ–∫–∞–∑–æ–≤ –≤ —ç—Ç–∏—Ö –ñ–ö –Ω–µ –±—ã–ª–æ.",
            parse_mode="Markdown"
        )
        return
    
    # –ï—Å—Ç—å –ø–æ–∫–∞–∑—ã ‚Äî —Ñ–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –ø–æ –ñ–ö
    response_parts = [f"üìç **{result['district']}** ({result['city']})\n"]
    response_parts.append(f"–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {result['days']} –¥–Ω–µ–π:\n")
    
    for obj_name, brokers in result["by_object"].items():
        brokers_str = ", ".join(brokers)
        response_parts.append(f"\nüè† **{obj_name}**:\n{brokers_str}")
    
    response_parts.append(f"\n\n_–í—Å–µ–≥–æ –±—Ä–æ–∫–µ—Ä–æ–≤: {result['total_brokers']}_")
    
    response = "".join(response_parts)
    
    # Telegram –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ ‚Äî 4096 —Å–∏–º–≤–æ–ª–æ–≤
    if len(response) > 4000:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞—Å—Ç—è–º–∏
        await update.message.reply_text(
            f"üìç **{result['district']}** ({result['city']})\n\n"
            f"–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {result['days']} –¥–Ω–µ–π –Ω–∞–π–¥–µ–Ω–æ {len(result['by_object'])} –ñ–ö —Å –ø–æ–∫–∞–∑–∞–º–∏.\n"
            f"–í—Å–µ–≥–æ –±—Ä–æ–∫–µ—Ä–æ–≤: {result['total_brokers']}",
            parse_mode="Markdown"
        )
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª–∏ –ø–æ —á–∞—Å—Ç—è–º
        chunk = ""
        for obj_name, brokers in result["by_object"].items():
            brokers_str = ", ".join(brokers)
            line = f"üè† **{obj_name}**:\n{brokers_str}\n\n"
            
            if len(chunk) + len(line) > 3500:
                await update.message.reply_text(chunk, parse_mode="Markdown")
                chunk = line
            else:
                chunk += line
        
        if chunk:
            await update.message.reply_text(chunk, parse_mode="Markdown")
    else:
        await update.message.reply_text(response, parse_mode="Markdown")


def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    token = os.getenv("TELEGRAM_BOT_TOKEN")
    
    if not token:
        print("‚ùå –û—à–∏–±–∫–∞: TELEGRAM_BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
        print("–î–æ–±–∞–≤—å —Ç–æ–∫–µ–Ω –≤ —Ñ–∞–π–ª .env")
        return
    
    print("ü§ñ –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞...")
    
    # –°–æ–∑–¥–∞—ë–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    app = Application.builder().token(token).build()
    
    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("help", help_command))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º
    print("‚úÖ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω! –ù–∞–∂–º–∏ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.")
    app.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == "__main__":
    main()
